################################################################################
# Setup
################################################################################

# Load required packages
library(caret)
library(dplyr)
library(earth)
library(gbm)
library(ggplot2)
library(kernlab)
library(mlbench)
library(NeuralNetTools)
library(nnet)
library(pdp)
library(randomForest)
library(vip)

# Colors
set1 <- RColorBrewer::brewer.pal(9, "Set1")

# Simulate data from the regression problems described in Friedman (1991) and
# Breiman (1996)
set.seed(3101)
trn1 <- as.data.frame(mlbench.friedman1(500))
trn2 <- as.data.frame(mlbench.friedman2(500))
trn3 <- as.data.frame(mlbench.friedman3(500))

# Load the (corrected) Boston housing data
data(boston, package = "pdp")

# Load concrete data
conc <- read.csv("Concrete_Data.csv", header = TRUE)

# Width of bars in each plot
bar.width <- 0.5


################################################################################
# Random forest analysis of the Boston housing data
################################################################################

# Fit a random forest to the Boston Housing data (mtry was tuned using cross-
# validation)
set.seed(101)
boston.rf <- randomForest(cmedv ~ ., data = boston, mtry = 6, ntree = 1000,
                          importance = TRUE)

# Figure ?
imp1 <- importance(boston.rf, type = 1) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Variable")
imp2 <- importance(boston.rf, type = 2) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Variable")
p1 <- ggplot(imp1, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`)) +
  geom_col() +
  coord_flip() +
  xlab("") +
  theme_light()
p2 <- ggplot(imp2, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
  geom_col() +
  coord_flip() +
  xlab("") +
  theme_light()
pdf(file = "manuscript-methodology\\boston-rf-vip.pdf", width = 8, height = 5)
grid.arrange(p1, p2, ncol = 2)
dev.off()


# Partial dependence plots for lstat, rm, and zn
pd1 <- partial(boston.rf, pred.var = "lstat")
pd2 <- partial(boston.rf, pred.var = "rm")
pd3 <- partial(boston.rf, pred.var = "zn")
pd.range <- range(c(pd1$yhat, pd2$yhat, pd3$yhat))
p1 <- autoplot(pd1) +
  ylim(pd.range[1L], pd.range[2L]) +
  theme_light() +
  geom_hline(yintercept = mean(boston$cmedv), linetype = 2, col = set1[1L],
             alpha = 0.5) +
  ylab("Partial dependence")
p2 <- autoplot(pd2) +
  ylim(pd.range[1L], pd.range[2L]) +
  theme_light() +
  geom_hline(yintercept = mean(boston$cmedv), linetype = 2, col = set1[1L],
             alpha = 0.5) +
  ylab("Partial dependence")
p3 <- autoplot(pd3) +
  ylim(pd.range[1L], pd.range[2L]) +
  theme_light() +
  geom_hline(yintercept = mean(boston$cmedv), linetype = 2, col = set1[1L],
             alpha = 0.5) +
  ylab("Partial dependence")

# Figure ?
pdf(file = "boston-rf-pdps.pdf", width = 12, height = 4)
grid.arrange(p1, p2, p3, ncol = 3)
dev.off()

# Variable importance scores (partial dependence)
boston.rf.vi <- vi(boston.rf, pred.var = names(subset(boston, select = -cmedv)))
p <- ggplot(boston.rf.vi, aes(x = reorder(Variable, -Importance), y = Importance)) +
  geom_col() +
  xlab("") +
  theme_light()

# Variable importance plots
p1 <- vip(boston.rf, pred.var = names(subset(boston, select = -cmedv)), FUN = sd)
p2 <- vip(boston.rf, pred.var = names(subset(boston, select = -cmedv)), FUN = mad)

# Figure ?
pdf(file = "manuscript-methodology\\boston-rf-vip-pd.pdf", width = 7, height = 4)
print(p)
dev.off()


################################################################################
# Boston housing example
################################################################################

ctrl <- trainControl(method = "cv", number = 5, verboseIter = TRUE)
set.seed(4578)
boston.nn.tune <- train(
  x = subset(boston, select = -cmedv),
  y = boston$cmedv,
  method = "nnet",
  linout = TRUE,
  maxit = 1000,
  trControl = ctrl,
  tuneGrid = expand.grid(size = 1:20, decay = c(0, 0.0001, 0.001, 0.01, 0.1))
)
plot(boston.nn.tune)

set.seed
X <- subset(boston, select = -cmedv)
X$chas <- as.numeric(X$chas)
boston.svm.tune <- train(
  x = X,
  y = boston$cmedv,
  method = "svmRadialCost",
  preProc = c("center", "scale"),
  metric = "Rsquared",
  trControl = ctrl,
  tuneGrid = data.frame("C" = seq(from = 0.25, to = 100, length = 100))
)
plot(boston.svm.tune)

set.seed(301)
fit <- nnet(cmedv ~ ., data = boston, size = 15, decay = 0.1, linout = TRUE,
            maxit = 10000)
vip(fit, pred.var = names(subset(boston, select = -cmedv)), quantiles = TRUE, probs = 10:90/100)
vip(fit, pred.var = names(subset(boston, select = -cmedv)), FUN = var)
vip(fit, pred.var = names(subset(boston, select = -cmedv)), FUN = IQR)

fit <- boston.rf
pred.var <- names(subset(boston, select = -cmedv))
vip(fit, pred.var = pred.var, FUN = function(x) {
  # max(abs(x - mean(boston$cmedv)))
  sqrt(mean((x - mean(boston$cmedv)) ^ 2))
})


################################################################################
# Friedman 1
################################################################################

# Simulate the data
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench::mlbench.friedman1(n = 500, sd = 1))
tibble::glimpse(trn)


# Multivariate adaptive regression splines -------------------------------------

# Fit a MARS model
set.seed(101)
trn.mars <- earth(y ~ ., data = trn, degree = 3, pmethod = "exhaustive", nfold = 5)
vip.mars <- vip(trn.mars, pred.var = paste0("x.", 1:10))
vip.mars + theme_bw()
evimp(trn.mars)
#     nsubsets   gcv    rss
# x.4       15 100.0  100.0
# x.1       14  83.2   82.9
# x.2       14  83.2   82.9
# x.5       12  59.3   58.7
# x.3       10  43.5   42.8


# Stochastic gradient boosting -------------------------------------------------

# Fit a GBM
set.seed(101)
trn.gbm <- gbm(y ~ ., data = trn, distribution = "gaussian", n.trees = 10000,
               shrinkage = 0.1, interaction.depth = 2, bag.fraction = 1,
               train.fraction = 1, cv.folds = 5, verbose = TRUE)
best.iter <- gbm.perf(trn.gbm, method = "cv")
print(best.iter)

# Variable importance plots
summary(trn.gbm, n.trees = best.iter)
vip.gbm <- vip(trn.gbm, pred.var = paste0("x.", 1:10), n.trees = best.iter)
print(vip.gbm)


# Neural network ---------------------------------------------------------------

# # Setup for k-fold cross-validation
# ctrl <- trainControl(method = "cv", number = 5, verboseIter = TRUE)
# set.seed(103)
# trn.nn.tune <- train(
#   x = subset(trn, select = -y),
#   y = trn$y,
#   method = "nnet",
#   linout = TRUE,
#   maxit = 1000,
#   trControl = ctrl,
#   tuneGrid = expand.grid(size = 1:20, decay = c(0, 0.0001, 0.001, 0.01, 0.1))
# )
# plot(trn.nn.tune)
#    size decay     RMSE  Rsquared     RMSESD  RsquaredSD
# 39    8  0.01 1.205598 0.9443347 0.08825044 0.005865337

# Fit a neural network to the Firedman 1 data set
set.seed(103)
trn.nn <- nnet(y ~ ., data = trn, size = 8, linout = TRUE, decay = 0.01,
               maxit = 1000, trace = FALSE)

vip(trn.nn, pred.var = paste0("x.", 1:10), FUN = var)
vip(trn.nn, pred.var = paste0("x.", 1:10), FUN = mad)

# Figure ?
pdf(file = "manuscript-methodology\\network.pdf", width = 12, height = 6)
plotnet(trn.nn, circle_col = "lightgrey")
dev.off()

# VIP: partial dependence algorithm
p1 <- vip(trn.nn, use.partial = TRUE, pred.var = paste0("x.", 1:10)) +
  theme_light() +
  ylab("Importance (partial dependence)")

# VIP: Garson's algorithm
trn.nn.garson <- garson(trn.nn, bar_plot = FALSE) %>%
  tibble::rownames_to_column("Variable") %>%
  select(Variable, Importance = rel_imp)
p2 <- ggplot(trn.nn.garson, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  xlab("") +
  ylab("Importance (Garson's algorithm)") +
  coord_flip() +
  theme_light()

# VIP: Olden's algorithm
trn.nn.olden <- olden(trn.nn, bar_plot = FALSE) %>%
  tibble::rownames_to_column("Variable") %>%
  select(Variable, Importance = importance)
p3 <- ggplot(trn.nn.olden, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  xlab("") +
  ylab("Importance (Olden's algorithm)") +
  coord_flip() +
  theme_light()

# Figure ?
pdf(file = "manuscript-methodology\\network-vip.pdf", width = 12, height = 6)
grid.arrange(p1, p2, p3, ncol = 3)
dev.off()


# vint <- function(x) {
#   pd <- partial(trn.nn, pred.var = c(x[1L], x[2L]))
#   c(sd(tapply(pd$yhat, INDEX = pd[[x[1L]]], FUN = sd)),
#     sd(tapply(pd$yhat, INDEX = pd[[x[2L]]], FUN = sd)))
# }
# combns <- combn(paste0("x.", 1:10), m = 2)
# res <- plyr::aaply(combns, .margins = 2, .fun = vint, .progress = "text")
# plot(rowMeans(res), type = "h")
# int <- data.frame(x = paste0(combns[1L, ], "*", combns[2L, ]), y = rowMeans(res))
# int <- int[order(int$y, decreasing = TRUE), ]
# save(int, file = "interaction-statistics.RData")
load("interaction-statistics.RData")

pdf(file = "network-int.pdf", width = 8, height = 4)
labs <-  c(
  expression(x[1]*x[2]), expression(x[1]*x[3]), expression(x[3]*x[10]), 
  expression(x[1]*x[8]), expression(x[3]*x[8]), expression(x[4]*x[5]),
  expression(x[3]*x[4]), expression(x[1]*x[4]), expression(x[2]*x[4]),
  expression(x[1]*x[5])
)
ggplot(int[1:10, ], aes(reorder(x, -y), y)) +
  geom_col(width = 0.75) +
  xlab("") +
  ylab("Interaction") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
  scale_x_discrete("", labels = labs) +
  theme_light()
dev.off()


################################################################################
# Linear model
################################################################################

# Simulate data
n <- 1000
set.seed(101)
x1 <- runif(N, min = 0, max = 1)
x2 <- runif(N, min = 0, max = 1)
d <- data.frame(x1, x2, y = 1 + 3*x1 - 5*x2 + rnorm(n, sd = 0.1))
pairs(d)

# Fit a simple linear model
fit <- lm(Y ~ X1 + X2, data = d)

# Estimated and true partial dependence plots
pd1 <- partial(fit, pred.var = "X1")
pd2 <- partial(fit, pred.var = "X2")
pdf(file = "lm-pdps.pdf", width = 8, height = 4)
grid.arrange(
  autoplot(pd1, pdp.size = 3.2, alpha = 0.5) + 
    geom_abline(slope = 3, intercept = -3/2, col = "red") +
    xlab(expression(X[1])) +
    ylab("Partial dependence") +
    ylim(-2.5, 2.5) +
    theme_light(), 
  autoplot(pd2, pdp.size = 1.2, alpha = 0.5) + 
    geom_abline(slope = -5, intercept = 5/2, col = "red") +
    xlab(expression(X[2])) +
    ylab("Partial dependence") +
    ylim(-2.5, 2.5) +
    theme_light(), 
  ncol = 2
)
dev.off()


varImp(fit)
vi(fit, use.partial = TRUE)
5 / 3  # absolute ratio of slopes


################################################################################
# Example: concrete compressive strength
################################################################################

# Train an SVM to the spam data
data(spam, package = "kernlab")
X <- subset(spam, select = -type)
y <- spam$type
set.seed(1141)
tune.svm <- train(
  x = X,
  y = y,
  method = "svmRadial",
  prob.model = TRUE,
  metric = "Accuracy",
  trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE),
  # tuneGrid = expand.grid(neighbors = 0:9, committees = 1:20)
  tuneLength = 10
)
set.seed(1141)
tune.rf <- randomForest(type ~ ., data = spam, importance = TRUE)
# tune.rf <- train(
#   x = X,
#   y = y,
#   method = "rf",
#   importance = TRUE,
#   metric = "Accuracy",
#   trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE),
#   tuneGrid = expand.grid(mtry = 1:8)
# )
grid.arrange(plot(tune.svm), plot(tune.rf), ncol = 2)
p <- vip(tune.svm, pred.var = names(X), progress = "text")
grid.arrange(p, plot(varImp(tune.svm)), plot(varImp(tune.rf)), ncol = 3)
varImpPlot(tune.rf)
pdf(file = "spam-vip.pdf", width = 7, height = 5)
print(p)
dev.off()


################################################################################
# Ozone data
################################################################################

# Load data from the UCI website
url <- "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/LAozone.data"
ozone <- read.csv(url, header = TRUE)

# Setup for repeated k-fold cross-validation
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 10, 
                     verboseIter = TRUE)
set.seed(0151)
ozone.tune <- train(x = subset(ozone, select = -ozone),
                    y = ozone$ozone,
                    method = "ppr",
                    linout = TRUE,
                    trace = FALSE,
                    metric = "Rsquared",
                    trControl = ctrl,
                    tuneLength = 10)
plot(ozone.tune)  # plot tuning results

# Compare to random forest
set.seed(0156)
ozone.rf <- randomForest(ozone ~ ., data = ozone, importance = TRUE)
plot(ozone.rf)
varImpPlot(ozone.rf)

# Variable importance plots
grid.arrange(
  vip(ozone.tune, pred.var = names(subset(ozone, select = -ozone))),
  plot(varImp(ozone.tune)),
  vip(ozone.rf, pred.var = names(subset(ozone, select = -ozone))),
  ncol = 3
)


################################################################################
# Pima indians diabetes data
################################################################################

# Load data
data(pima, package = "pdp")
pima <- na.omit(pima)

# Setup for repeated k-fold cross-validation
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 10, 
                     classProbs = TRUE, summaryFunction = twoClassSummary,
                     verboseIter = TRUE)

# Tune the model
set.seed(1256)
pima.tune <- train(
  x = subset(pima, select = -diabetes),
  y = pima$diabetes,
  method = "nnet",
  trace = FALSE,
  maxit = 2000,
  metric = "ROC",
  trControl = ctrl,
  tuneLength = 5
)
plot(pima.tune)  # plot tuning results

# Variable importance plots
xnames <- names(subset(pima, select = -diabetes))
pdf(file = "pima-vip.pdf", width = 7, height = 5)
vip(pima.tune, pred.var = xnames)
dev.off()

pd.all <- NULL
for (i in 1:length(xnames)) {
  pd <- partial(pima.tune, pred.var = xnames[i])
  pd <- cbind(xnames[i], pd)
  names(pd) <- c("Feature", "X", "Y")
  pd.all <- rbind(pd.all, pd)
}

# Figure ?
pdf(file = "pima-pdps.pdf", width = 12, height = 4)
ggplot(pd.all, aes(x = X, y = Y)) +
  geom_line() +
  facet_grid( ~ Feature, scales = "free_x") +
  xlab("") +
  ylab("Partial dependence") +
  theme_light()
dev.off()

# Compare to random forest
set.seed(0156)
pima.rf <- randomForest(diabetes ~ ., data = na.omit(pima), importance = TRUE)
plot(pima.rf)
varImpPlot(pima.rf)
